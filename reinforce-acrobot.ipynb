{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "611037e7",
   "metadata": {},
   "source": [
    "# Reinforcement Learning : Acrobot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cfa558",
   "metadata": {},
   "source": [
    "## Description\n",
    "The system consists of two links connected linearly to form a chain, with one end of the chain fixed. The joint between the two links is actuated. The goal is to apply torques on the actuated joint to swing the free end of the linear chain above a given height while starting from the initial state of hanging downwards.\n",
    "\n",
    "### Action Space\n",
    "The action is discrete, deterministic, and represents the torque applied on the actuated joint between the two links.\n",
    "- 0 : apply -1 torque to the actuated joint\n",
    "- 1 : apply 0 torque to the actuated joint\n",
    "- 2 : apply 1 torque to the actuated joint\n",
    "\n",
    "### Observation Space\n",
    "The observation is a ndarray with shape (6,) that provides information about the two rotational joint angles as well as their angular velocities.\n",
    "- 0 : cosine of theta1 [-1, 1]\n",
    "- 1 : sine of theta1 [-1, 1]\n",
    "- 2 : cosine of theta2 [-1, 1]\n",
    "- 3 : sine of theta2 [-1, 1]\n",
    "- 4 : angular velocity of theta1 [-12.567 (-4 * pi), 12.567 (4 * pi)]\n",
    "- 5 : angular velocity of theta2 [-28.274 (-9 * pi), 28.274 (9 * pi)]\n",
    "\n",
    "### Starting State\n",
    "Each parameter in the underlying state (theta1, theta2, and the two angular velocities) is initialized uniformly between -0.1 and 0.1. This means both links are pointing downwards with some initial stochasticity.\n",
    "\n",
    "### Reward\n",
    "The goal is to have the free end reach a designated target height in as few steps as possible, and as such all steps that do not reach the goal incur a reward of -1. Achieving the target height results in termination with a reward of 0. The reward threshold is -100.\n",
    "\n",
    "### Episode End\n",
    "The episode ends if one of the following occurs:\n",
    "- Termination: The free end reaches the target height, which is constructed as: -cos(theta1) - cos(theta2 + theta1) > 1.0\n",
    "- Truncation: Episode length is greater than 500 (200 for v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "715d7873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tqdm\n",
    "import logging\n",
    "import itertools\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dist\n",
    "torch.manual_seed(0)\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "026e63df",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Acrobot-v1\")\n",
    "np.random.seed(42)\n",
    "observation, _ = env.reset(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f082450b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([ -1.        -1.        -1.        -1.       -12.566371 -28.274334], [ 1.        1.        1.        1.       12.566371 28.274334], (6,), float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2661529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe1d0a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in vars(env):\n",
    "    logging.info('%s: %s', key, vars(env)[key])\n",
    "for key in vars(env.spec):\n",
    "    logging.info('%s: %s', key, vars(env.spec)[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa1cca7",
   "metadata": {},
   "source": [
    "## A2C : Advantage Actor-Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de8aa6f",
   "metadata": {},
   "source": [
    "**A2C module by torch :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0146d667",
   "metadata": {},
   "outputs": [],
   "source": [
    "## A2C module\n",
    "class A2C:\n",
    "    ## initialization\n",
    "    def __init__(self, env, gamma=0.99):\n",
    "        self.gamma = gamma\n",
    "        self.discount = 1.\n",
    "        self.action_n = env.action_space.n\n",
    "        self.actor_net = self.build_net(input_size=env.observation_space.shape[0], \n",
    "                                        hidden_sizes=[100,],\n",
    "                                        output_size=env.action_space.n,\n",
    "                                        output_activator=nn.Softmax(1))\n",
    "        self.actor_optimizer = optim.Adam(self.actor_net.parameters(), 0.001)\n",
    "        self.critic_net = self.build_net(input_size=env.observation_space.shape[0], \n",
    "                                         hidden_sizes=[100,])\n",
    "        self.critic_optimizer = optim.Adam(self.critic_net.parameters(), 0.001)\n",
    "        self.critic_loss = nn.MSELoss()\n",
    "    \n",
    "    ## set A2C mode to None or train, default None\n",
    "    def reset_mode(self, mode=None):\n",
    "        self.mode = mode\n",
    "        if self.mode=='train':\n",
    "            self.traj = []\n",
    "            self.discount = 1.\n",
    "    \n",
    "    ## take in the current observation and reward -> take an action \n",
    "    def play_step(self, observation, reward, done):\n",
    "        state_tensor = torch.as_tensor(observation, dtype=torch.float).reshape(1, -1)\n",
    "        proba_tensor = self.actor_net(state_tensor)\n",
    "        action_tensor = dist.Categorical(proba_tensor).sample()\n",
    "        action = action_tensor.numpy()[0]\n",
    "        \n",
    "        if self.mode=='train':\n",
    "            self.traj += [observation, reward, done, action]\n",
    "            if len(self.traj)>=8:\n",
    "                self.reinforce()\n",
    "            self.discount *= self.gamma\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    ## build a neural network with ReLU activation function\n",
    "    def build_net(self, input_size, hidden_sizes, output_size=1, \n",
    "                  output_activator=None):\n",
    "        layers=[]\n",
    "        \n",
    "        # build layers\n",
    "        for input_size, output_size in zip(\n",
    "                [input_size,]+hidden_sizes, hidden_sizes+[output_size,]):\n",
    "            layers.append(nn.Linear(input_size, output_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers = layers[:-1]\n",
    "        if output_activator:\n",
    "            layers.append(output_activator)\n",
    "            \n",
    "        #build network\n",
    "        net = nn.Sequential(*layers)\n",
    "        \n",
    "        return net\n",
    "    \n",
    "    # implementation of reinforce algorithme \n",
    "    def reinforce(self):\n",
    "        state, _, _, action, next_state, reward, done, next_action = self.traj[-8:]\n",
    "        state_tensor = torch.as_tensor(state, dtype=torch.float).unsqueeze(0)\n",
    "        next_state_tensor = torch.as_tensor(next_state, dtype=torch.float).unsqueeze(0)\n",
    "        \n",
    "        # TD error\n",
    "        next_v_tensor = self.critic_net(next_state_tensor)\n",
    "        target_tensor = reward + (1.-done)*self.gamma*next_v_tensor\n",
    "        v_tensor = self.critic_net(state_tensor)\n",
    "        td_error_tensor = target_tensor - v_tensor\n",
    "        \n",
    "        # train actor network\n",
    "        # proba of taking current action at current state\n",
    "        pi_tensor = self.actor_net(state_tensor)[0, action]\n",
    "        logpi_tensor = torch.log(pi_tensor.clamp(1e-6, 1.))\n",
    "        actor_loss_tensor = -(self.discount*td_error_tensor*logpi_tensor).squeeze()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        # gradients of the actor loss\n",
    "        actor_loss_tensor.backward(retain_graph=True)\n",
    "        # update the parameters of actor network using gradients\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # train critic network\n",
    "        pred_tensor = self.critic_net(state_tensor)\n",
    "        critic_loss_tensor = self.critic_loss(pred_tensor, target_tensor)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        # gradients of the critic loss\n",
    "        critic_loss_tensor.backward()\n",
    "        # update the parameters of critic network using gradients\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "## simulate an episode\n",
    "## return the total reward of the episode and elapsed steps\n",
    "def play_epis(env, agent, max_episode_steps=None, \n",
    "              mode=None, render=False):\n",
    "    observation = env.reset()\n",
    "    reward = 0.\n",
    "    done = False\n",
    "    agent.reset_mode(mode=mode)\n",
    "    episode_reward = 0.\n",
    "    elapsed_steps = 0\n",
    "    \n",
    "    while True:\n",
    "        action = agent.play_step(observation, reward, done)\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        elapsed_steps += 1\n",
    "        if max_episode_steps and elapsed_steps>=max_episode_steps:\n",
    "            break\n",
    "            \n",
    "        agent.close()\n",
    "            \n",
    "        return episode_reward, elapsed_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fda5a9b",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5204a559",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG, \n",
    "                    format='%(asctime)s [%(levelname)s] %(message)s', \n",
    "                    stream=sys.stdout, \n",
    "                    datefmt='%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e74894c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leche\\AppData\\Local\\Temp\\ipykernel_66376\\3144375505.py:27: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  state_tensor = torch.as_tensor(observation, dtype=torch.float).reshape(1, -1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 6 at dim 1 (got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m episode_rewards \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mcount():\n\u001b[1;32m----> 7\u001b[0m     episode_reward, elapsed_steps \u001b[38;5;241m=\u001b[39m \u001b[43mplay_epis\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mmax_episode_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_episode_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     episode_rewards\u001b[38;5;241m.\u001b[39mappend(episode_reward)\n\u001b[0;32m     11\u001b[0m     logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain episode \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m: reward = \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124m, steps = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     12\u001b[0m                   episode, episode_reward, elapsed_steps)\n",
      "Cell \u001b[1;32mIn[7], line 103\u001b[0m, in \u001b[0;36mplay_epis\u001b[1;34m(env, agent, max_episode_steps, mode, render)\u001b[0m\n\u001b[0;32m    100\u001b[0m elapsed_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 103\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m render:\n\u001b[0;32m    105\u001b[0m         env\u001b[38;5;241m.\u001b[39mrender()\n",
      "Cell \u001b[1;32mIn[7], line 27\u001b[0m, in \u001b[0;36mA2C.play_step\u001b[1;34m(self, observation, reward, done)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplay_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation, reward, done):\n\u001b[1;32m---> 27\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     28\u001b[0m     proba_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_net(state_tensor)\n\u001b[0;32m     29\u001b[0m     action_tensor \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mCategorical(proba_tensor)\u001b[38;5;241m.\u001b[39msample()\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 6 at dim 1 (got 0)"
     ]
    }
   ],
   "source": [
    "agent = A2C(env)\n",
    "\n",
    "## train\n",
    "logging.info('==== train ====')\n",
    "episode_rewards = []\n",
    "for episode in itertools.count():\n",
    "    episode_reward, elapsed_steps = play_epis(env, agent, \n",
    "                                         max_episode_steps=env._max_episode_steps, \n",
    "                                         mode='train', render=1)\n",
    "    episode_rewards.append(episode_reward)\n",
    "    logging.debug('train episode %d: reward = %.2f, steps = %d', \n",
    "                  episode, episode_reward, elapsed_steps)\n",
    "    if np.mean(episode_rewards[-10:])>-120:\n",
    "        break\n",
    "plt.plot(episode_rewards)\n",
    "\n",
    "## test\n",
    "logging.info('==== test ====')\n",
    "episode_rewards = []\n",
    "for episode in range(100):\n",
    "    episode_reward, elapsed_steps = play_epis(env, agent)\n",
    "    episode_rewards.append(episode_reward)\n",
    "    logging.debug('test episode %d: reward = %.2f, steps = %d', \n",
    "                  episode, episode_reward, elapsed_steps)\n",
    "logging.info('average episode reward = %.2f', \n",
    "             np.mean(episode_rewards))\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
