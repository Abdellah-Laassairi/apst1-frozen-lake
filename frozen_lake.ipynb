{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installation\n",
    "\n",
    "The simplest way to install gymnasium is to use *pip*. This is easily done, apart from a slight option activation to get all the available environments installed, using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasium\n",
      "  Using cached gymnasium-0.27.1-py3-none-any.whl (883 kB)\n",
      "Collecting jax-jumpy>=0.2.0\n",
      "  Using cached jax_jumpy-0.2.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/odeck/miniconda3/envs/mnp/lib/python3.8/site-packages (from gymnasium) (1.23.5)\n",
      "Collecting gymnasium-notices>=0.0.1\n",
      "  Using cached gymnasium_notices-0.0.1-py3-none-any.whl (2.8 kB)\n",
      "Collecting cloudpickle>=1.2.0\n",
      "  Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/odeck/miniconda3/envs/mnp/lib/python3.8/site-packages (from gymnasium) (4.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/odeck/miniconda3/envs/mnp/lib/python3.8/site-packages (from gymnasium) (4.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/odeck/miniconda3/envs/mnp/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.8.0)\n",
      "Installing collected packages: gymnasium-notices, jax-jumpy, cloudpickle, gymnasium\n",
      "Successfully installed cloudpickle-2.2.1 gymnasium-0.27.1 gymnasium-notices-0.0.1 jax-jumpy-0.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import pygame\n",
    "import sys\n",
    "import argparse\n",
    "from tools.qlearning import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table = np.fromfile(\"data/q_table.dat\", dtype=float)\n",
    "q_table.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also have to install the *pygame* library as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interacting with the environment\n",
    "\n",
    "The gymnasium library is a collection of test problems, often called **environments**, that you can use to work out your reinforcement learning algorithms. These environments have a shared interface, allowing you to write general algorithms. Let's have a look at the CartPole environments and play a bunch of games.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "num_actions = env.action_space.n\n",
    "num_states = env.observation_space.n\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros((16, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 0.0, False, False, {'prob': 0.3333333333333333})\n",
      "(0, 0.0, False, False, {'prob': 0.3333333333333333})\n",
      "(4, 0.0, False, False, {'prob': 0.3333333333333333})\n",
      "(5, 0.0, True, False, {'prob': 0.3333333333333333})\n",
      "(4, 0.0, False, False, {'prob': 0.3333333333333333})\n",
      "(0, 0.0, False, False, {'prob': 0.3333333333333333})\n",
      "(1, 0.0, False, False, {'prob': 0.3333333333333333})\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "video system not initialized",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m observation, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset(seed\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m    keys \u001b[39m=\u001b[39m pygame\u001b[39m.\u001b[39;49mkey\u001b[39m.\u001b[39;49mget_pressed()\n\u001b[1;32m      6\u001b[0m    action \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39msample()  \u001b[39m# this is where you would insert your policy\u001b[39;00m\n\u001b[1;32m      7\u001b[0m    observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n",
      "\u001b[0;31merror\u001b[0m: video system not initialized"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "while True:\n",
    "   keys = pygame.key.get_pressed()\n",
    "   action = env.action_space.sample()  # this is where you would insert your policy\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "   result = env.step(action)\n",
    "   print(result)\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "   env.reset()\n",
    "   if keys[pygame.K_ESCAPE]:\n",
    "      env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = env.action_space.n\n",
    "num_states = env.observation_space.n\n",
    "q_table = np.zeros((num_states, num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from tqdm import *\n",
    "\n",
    "\n",
    "def q_train_greedy(env, alpha=0.9, gamma=0.95, max_epsilon=1, min_epsilon=0.001, decay_rate=0.001, max_n_steps=100, n_episodes=100):\n",
    "    \"\"\" Qâ€“learning algorithm (epsilon-greedy)\n",
    "    \n",
    "    \"\"\"\n",
    "    num_actions = env.action_space.n\n",
    "    num_states = env.observation_space.n\n",
    "    q_table = np.zeros((num_states, num_actions))\n",
    "\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    epsilon = max_epsilon\n",
    "\n",
    "    print()\n",
    "    print(\"Starting Q-learning algorithm...\")\n",
    "    for episode in trange(n_episodes):\n",
    "        s = env.reset()\n",
    "        total_reward = 0\n",
    "        for i in range(max_n_steps):\n",
    "            U = np.random.uniform(0, 1)\n",
    "            if U < epsilon:\n",
    "                a = env.action_space.sample() # selecting action a at random from A \n",
    "            else:\n",
    "                a = np.argmax(q_table[s]) # Select action a given X following policy derived from q;\n",
    "            \n",
    "            s_new, r, done, _ , _= env.step(a)\n",
    "            q_table[s, a] = (1-alpha)*q_table[s, a] + alpha*(r + gamma*np.max(q_table[s_new]) - q_table[s, a])\n",
    "            \n",
    "            s, total_reward = s_new, total_reward+r\n",
    "\n",
    "            # if X is a terminal state then go to next episode;\n",
    "            if done: \n",
    "                rewards.append(total_reward) \n",
    "                epsilon = min_epsilon + (max_epsilon-min_epsilon)*np.exp(-decay_rate*episode)\n",
    "                break\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Q-learning algorithm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m q_train_greedy(env)\n",
      "Cell \u001b[0;32mIn[3], line 20\u001b[0m, in \u001b[0;36mq_train_greedy\u001b[0;34m(env, alpha, gamma, max_epsilon, min_epsilon, decay_rate, max_n_steps, n_episodes)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStarting Q-learning algorithm...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m episode \u001b[39min\u001b[39;00m trange(n_episodes):\n\u001b[0;32m---> 20\u001b[0m     s \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mreset()\n\u001b[1;32m     21\u001b[0m     total_reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     22\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_n_steps):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/time_limit.py:69\u001b[0m, in \u001b[0;36mTimeLimit.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39m\"\"\"Resets the environment with :param:`**kwargs` and sets the number of steps elapsed to zero.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39m    The reset environment\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mreset(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/order_enforcing.py:43\u001b[0m, in \u001b[0;36mOrderEnforcing.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mreset(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/env_checker.py:47\u001b[0m, in \u001b[0;36mPassiveEnvChecker.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[39mreturn\u001b[39;00m env_reset_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mreset(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/envs/toy_text/frozen_lake.py:322\u001b[0m, in \u001b[0;36mFrozenLakeEnv.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlastaction \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 322\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[1;32m    323\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ms), {\u001b[39m\"\u001b[39m\u001b[39mprob\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1\u001b[39m}\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/envs/toy_text/frozen_lake.py:338\u001b[0m, in \u001b[0;36mFrozenLakeEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_render_text()\n\u001b[1;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[0;32m--> 338\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render_gui(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_mode)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/envs/toy_text/frozen_lake.py:408\u001b[0m, in \u001b[0;36mFrozenLakeEnv._render_gui\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    405\u001b[0m pos \u001b[39m=\u001b[39m (x \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcell_size[\u001b[39m0\u001b[39m], y \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcell_size[\u001b[39m1\u001b[39m])\n\u001b[1;32m    406\u001b[0m rect \u001b[39m=\u001b[39m (\u001b[39m*\u001b[39mpos, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcell_size)\n\u001b[0;32m--> 408\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwindow_surface\u001b[39m.\u001b[39;49mblit(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mice_img, pos)\n\u001b[1;32m    409\u001b[0m \u001b[39mif\u001b[39;00m desc[y][x] \u001b[39m==\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mH\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    410\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_surface\u001b[39m.\u001b[39mblit(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhole_img, pos)\n",
      "\u001b[0;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "q_train_greedy(env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
